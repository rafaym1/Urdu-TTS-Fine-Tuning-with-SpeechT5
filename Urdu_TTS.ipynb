
# Fine-tuning SpeechT5 for multilingual TTS
"""

!pip install datasets soundfile speechbrain

!pip install git+https://github.com/huggingface/transformers.git

!pip install --upgrade accelerate

"""## Load the model

We'll start from SpeechT5 that's already been fine-tuned for English TTS, and fine-tune it again but for a new language. For more info about the original checkpoint, you can find its model card on the [Hugging Face Hub](https://huggingface.co/microsoft/speecht5_tts).
"""

from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech

processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")

from huggingface_hub import notebook_login

notebook_login()

from datasets import load_dataset, Audio

dataset = load_dataset(
    "mozilla-foundation/common_voice_13_0", "ur", split="train"
)

dataset

"""It's important to set the sampling rate of the audio data to 16 kHz, which is what SpeechT5 expects."""

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

"""Transliteration Urdu to Roman Urdu"""

buck2uni = {
            u"\u0627":"A",
            u"\u0627":"A",
            u"\u0675":"A",
            u"\u0673":"A",
            u"\u0630":"A",
            u"\u0622":"AA",
            u"\u0628":"B",
            u"\u067E":"P",
            u"\u062A":"T",
            u"\u0637":"T",
            u"\u0679":"T",
            u"\u062C":"J",
            u"\u0633":"S",
            u"\u062B":"S",
            u"\u0635":"S",
            u"\u0686":"CH",
            u"\u062D":"H",
            u"\u0647":"H",
            u"\u0629":"H",
            u"\u06DF":"H",
            u"\u062E":"KH",
            u"\u062F":"D",
            u"\u0688":"D",
            u"\u0630":"Z",
            u"\u0632":"Z",
            u"\u0636":"Z",
            u"\u0638":"Z",
            u"\u068E":"Z",
            u"\u0631":"R",
            u"\u0691":"R",
            u"\u0634":"SH",
            u"\u063A":"GH",
            u"\u0641":"F",
            u"\u06A9":"K",
            u"\u0642":"K",
            u"\u06AF":"G",
            u"\u0644":"L",
            u"\u0645":"M",
            u"\u0646":"N",
            u"\u06BA":"N",
            u"\u0648":"O",
            u"\u0649":"Y",
            u"\u0626":"Y",
            u"\u06CC":"Y",
            u"\u06D2":"E",
            u"\u06C1":"H",
            u"\u064A":"E"  ,
            u"\u06C2":"AH"  ,
            u"\u06BE":"H"  ,
            u"\u0639":"A"  ,
            u"\u0643":"K" ,
            u"\u0621":"A",
            u"\u0624":"O",
            u"\u060C":"" #seperator ulta comma
}
def transString(string, reverse=0):
    '''Given a Unicode string, transliterate into Buckwalter. To go from
    Buckwalter back to Unicode, set reverse=1'''
    for k, v in buck2uni.items():
      if not reverse:
            string = string.replace(k, v)
      else:
            string = string.replace(v, k)
    return string
print(dataset[2]["sentence"])
print(transString(dataset[2]["sentence"]))

tokenizer = processor.tokenizer

def extract_all_chars(batch):
    all_text = " ".join(batch["sentence"])
    vocab = list(set(all_text))
    return {"vocab": [vocab], "all_text": [all_text]}

vocabs = dataset.map(
    extract_all_chars,
    batched=True,
    batch_size=-1,
    keep_in_memory=True,
    remove_columns=dataset.column_names,
)

dataset_vocab = set(vocabs["vocab"][0])
tokenizer_vocab = {k for k,_ in tokenizer.get_vocab().items()}

"""Now we have two sets of characters, one with the vocabulary from the dataset and one with the vocabulary from the tokenizer. By taking the difference between these sets, we find the characters that are in the dataset but not in the tokenizer."""

dataset_vocab - tokenizer_vocab

"""Next, we will define a function to map these characters to valid tokens and then run it on the dataset. No need to handle space, that's already replaced by `▁` in the tokenizer."""

replacements = [
  (' ', ' '),
  ('،', ','),
  ('ؓ', ''),
  ('ؔ', ''),
  ('؛', ';'),
  ('؟', '?'),
  ('ء', ''),
  ('آ', 'aa'),
  ('أ', 'aa'),
  ('ؤ', ''),
  ('ئ', ''),
  ('ا', 'aa'),
  ('ب', 'b'),
  ('ت', 't'),
  ('ث', 's'),
  ('ج', 'j'),
  ('ح', 'h'),
  ('خ', 'kh'),
  ('د', 'd'),
  ('ذ', 'z'),
  ('ر', 'r'),
  ('ز', 'z'),
  ('س', 's'),
  ('ش', 'sh'),
  ('ص', 's'),
  ('ض', 'd'),
  ('ط', 't'),
  ('ظ', 'z'),
  ('ع', "'"),
  ('غ', 'gh'),
  ('ف', 'f'),
  ('ق', 'q'),
  ('ك', 'k'),
  ('ل', 'l'),
  ('م', 'm'),
  ('ن', 'n'),
  ('ه', 'h'),
  ('و', 'w'),
  ('ي', 'y'),
  ('ً', 'an'),
  ('َ', 'a'),
  ('ُ', 'u'),
  ('ِ', 'i'),
  ('ّ', ''),
  ('ْ', ''),
  ('ٓ', ''),
  ('ٔ', ''),
  ('ٰ', ''),
  ('ٹ', 't'),
  ('پ', 'p'),
  ('چ', 'ch'),
  ('ڈ', 'd'),
  ('ڑ', 'r'),
  ('ژ', 'zh'),
  ('ک', 'k'),
  ('گ', 'g'),
  ('ں', 'n'),
  ('ھ', 'h'),
  ('ہ', 'h'),
  ('ۂ', 'h'),
  ('ۃ', 't'),
  ('ی', 'y'),
  ('ے', 'e'),
  ('ۓ', 'e'),
  ('۔', '.'),
  ('‘', "'"),
  ('’', "'"),
  ('“', '"'),
  ('”', '"'),
  ('…', '...'),
  ('ﭨ', ''),
  ('ﮭ', ''),
  ('ﮯ', ''),
  ('ﯾ', ''),
  ('ﷲ', 'Allah'),
  ('ﷺ', 'Sallallahu alayhi wa sallam'),
  ('ﺗ', 't'),
  ('ﺘ', 't'),
  ('ﺩ', 'd'),
  ('ﺲ', 's'),
  ('ﻧ', 'n'),
  ('ﻮ', 'w')
]

def cleanup_text(inputs):
    for src, dst in replacements:
        inputs["sentence"] = inputs["sentence"].replace(src, dst)
    return inputs

dataset = dataset.map(cleanup_text)



"""## Speaker embeddings

To allow the TTS model to distinguish between multiple speakers, we'll need to create a speaker embedding for each example. The speaker embedding is simply an additional input into the model that captures a particular speaker's voice characteristics.

To create the speaker embeddings, we use the [spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb) model from SpeechBrain. The following defines a function `create_speaker_embedding()` that takes an input audio waveform and outputs a 512-element vector containing the corresponding speaker embedding.
"""

import os
import torch
from speechbrain.pretrained import EncoderClassifier

spk_model_name = "speechbrain/spkrec-xvect-voxceleb"

device = "cpu"
speaker_model = EncoderClassifier.from_hparams(
    source=spk_model_name,
    run_opts={"device": device},
    savedir=os.path.join("/tmp", spk_model_name)
)

def create_speaker_embedding(waveform):
    with torch.no_grad():
        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))
        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)
        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()
    return speaker_embeddings

"""Note: the `speechbrain/spkrec-xvect-voxceleb` model was trained on English speech from the VoxCeleb dataset, while our training examples are in Urdu. We're assuming here that this model will still make reasonable speaker embeddings, but this may not be true. First training an X-vector model on the target speech, such as Urdu, might work better.

## Preparing the dataset

The `prepare_dataset` function takes in a single example and uses the `SpeechT5Processor` object to tokenize the input text and load the target audio into a log-mel spectrogram. It also adds the speaker embeddings as an additional input.
"""

def prepare_dataset(example):
    # load the audio data; if necessary, this resamples the audio to 16kHz
    audio = example["audio"]

    # feature extraction and tokenization
    example = processor(
        text=example["sentence"],
        audio_target=audio["array"],
        sampling_rate=audio["sampling_rate"],
        return_attention_mask=False,
    )

    # strip off the batch dimension
    example["labels"] = example["labels"][0]

    # use SpeechBrain to obtain x-vector
    example["speaker_embeddings"] = create_speaker_embedding(audio["array"])

    return example

"""Let's verify the processing is correct by looking at a single example:"""

processed_example = prepare_dataset(dataset[0])

"""This should give us:

- the tokens for the input text in `input_ids`
- the speaker embedding in `speaker_embeddings`
- the target spectrogram in `labels`
"""

list(processed_example.keys())

"""The tokens should decode into the original text, with `</s>` to mark the end of the sentence."""

tokenizer.decode(processed_example["input_ids"])

"""Speaker embeddings should be a 512-element vector:"""

processed_example["speaker_embeddings"].shape

"""The labels should be a log-mel spectrogram with 80 mel bins."""

import matplotlib.pyplot as plt
plt.figure()
plt.imshow(processed_example["labels"].T)
plt.show()

"""If we run a vocoder on the log-mel spectrogram, it should produce the original audio again. We'll load the HiFi-GAN vocoder from the original [SpeechT5 checkpoint](https://hf.co/microsoft/speecht5_hifigan)."""

from transformers import SpeechT5HifiGan
vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")

spectrogram = torch.tensor(processed_example["labels"])
with torch.no_grad():
    speech = vocoder(spectrogram)

from IPython.display import Audio
Audio(speech.cpu().numpy(), rate=16000)

"""That all looks and sounds good! We can now process the entire dataset. This will take between 5 and 10 minutes."""

dataset = dataset.map(
    prepare_dataset, remove_columns=dataset.column_names,
)

"""Some of the examples in the dataset are apparently longer than the maximum input length the model can handle (600 tokens), so we should remove those from the dataset. In fact, to allow for larger batch sizes we'll remove anything over 200 tokens."""

def is_not_too_long(input_ids):
    input_length = len(input_ids)
    return input_length < 200

dataset = dataset.filter(is_not_too_long, input_columns=["input_ids"])

"""How many examples are left?"""

len(dataset)

"""## Train/test split

Create a basic train/test split. For our purposes, it's OK if the same speaker is part of both sets.
"""

dataset = dataset.train_test_split(test_size=0.1)

"""What does the dataset look like now?"""

dataset

"""## Collator to make batches

We need to define a custom collator to combine multiple examples into a batch. This will pad shorter sequences with padding tokens. For the spectrogram labels, the padded portions are replaced with the special value -100. This special value tells the model to ignore that part of the spectrogram when calculating the spectrogram loss.

"""

from dataclasses import dataclass
from typing import Any, Dict, List, Union

@dataclass
class TTSDataCollatorWithPadding:
    processor: Any

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:

        input_ids = [{"input_ids": feature["input_ids"]} for feature in features]
        label_features = [{"input_values": feature["labels"]} for feature in features]
        speaker_features = [feature["speaker_embeddings"] for feature in features]

        # collate the inputs and targets into a batch
        batch = processor.pad(
            input_ids=input_ids,
            labels=label_features,
            return_tensors="pt",
        )

        # replace padding with -100 to ignore loss correctly
        batch["labels"] = batch["labels"].masked_fill(
            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100
        )

        # not used during fine-tuning
        del batch["decoder_attention_mask"]

        # round down target lengths to multiple of reduction factor
        if model.config.reduction_factor > 1:
            target_lengths = torch.tensor([
                len(feature["input_values"]) for feature in label_features
            ])
            target_lengths = target_lengths.new([
                length - length % model.config.reduction_factor for length in target_lengths
            ])
            max_length = max(target_lengths)
            batch["labels"] = batch["labels"][:, :max_length]

        # also add in the speaker embeddings
        batch["speaker_embeddings"] = torch.tensor(speaker_features)

        return batch

"""In SpeechT5, the input to the decoder part of the model is reduced by a factor 2. In other words, it throws away every other timestep from the target sequence. The decoder then predicts a sequence that is twice as long. Since the original target sequence length may be odd, the data collator makes sure to round the maximum length of the batch down to be a multiple of 2."""

data_collator = TTSDataCollatorWithPadding(processor=processor)

"""Let's test the data collator."""

features = [
    dataset["train"][0],
    dataset["train"][1],
    dataset["train"][20],
]

batch = data_collator(features)

{k:v.shape for k,v in batch.items()}

"""Looks good!

## Training

It's always a good idea to upload model checkpoints directly to the [Hugging Face Hub](https://huggingface.co/) while training. To allow this, first log in to the Hub by entering your Hub authentication token:
"""

from huggingface_hub import notebook_login

notebook_login()

"""The `use_cache=True` option is incompatible with gradient checkpointing. Disable it for training, otherwise it keeps complaining about it."""

model.config.use_cache = False

"""Define the training arguments.

We won't be computing any evaluation metrics during the training process. Instead, we'll only look at the loss. The lower the loss, the better the model.

Note: If you do not want to upload the model checkpoints to the Hub, set `push_to_hub=False`.
"""

from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./speecht5_tts_commonvoice_ur",  # change to a repo name of your choice
    per_device_train_batch_size=16,
    gradient_accumulation_steps=2,
    learning_rate=1e-5,
    warmup_steps=500,
    max_steps=4000,
    gradient_checkpointing=True,
    # fp16=True,
    evaluation_strategy="steps",
    per_device_eval_batch_size=8,
    save_steps=1000,
    eval_steps=1000,
    logging_steps=25,
    report_to=["tensorboard"],
    load_best_model_at_end=True,
    greater_is_better=False,
    label_names=["labels"],
    push_to_hub=True,
)

"""Create the trainer object using the model, dataset, and data collator."""

from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    args=training_args,
    model=model,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    data_collator=data_collator,
    tokenizer=processor.tokenizer,
)

"""And with that, we're ready to start training!
It is possible that you will encounter a CUDA "out-of-memory" error when you start training. In this case, you can reduce the `per_device_train_batch_size` incrementally by factors of 2 and increase `gradient_accumulation_steps` by 2x to compensate.
"""

trainer.train()

"""If we do one more `push_to_hub()` after training we can get a nice model card built for us. We simply have to set the appropriate keyword arguments (kwargs). You can change these values to match your dataset, language and model name accordingly:"""

kwargs = {
    "dataset_tags": "mozilla-foundation/common_voice_13_0",
    "dataset": "common_voice_13_0",  # a 'pretty' name for the training dataset
    "dataset_args": "config: ur, split: train",
    "language": "ur",
    "model_name": "SpeechT5 TTS Urdu",  # a 'pretty' name for your model
    "finetuned_from": "microsoft/speecht5_tts",
    "tasks": "text-to-speech",
    "tags": "",
}

"""The training results can now be uploaded to the Hub. To do so, execute the `push_to_hub` command:"""

trainer.push_to_hub(**kwargs)

"""You can now share this model with anyone using the link on the Hub.

## Evaluate

After training finishes, let's use the model to synthesize some speech!

I'm loading the model from the Hugging Face Hub, as the Colab notebook was terminated before training finished (which is why it's a good idea to use `push_to_hub=True` when training).
"""

model = SpeechT5ForTextToSpeech.from_pretrained("ZainabShah02/speecht5_tts_commonvoice_ur")

"""First we need to obtain a speaker embedding. We can simply grab one from the test set."""

example = dataset["test"][304]
speaker_embeddings = torch.tensor(example["speaker_embeddings"]).unsqueeze(0)
speaker_embeddings.shape

"""Define some input text and tokenize it."""

text = "Aapka kia haal hay"

#tokenizer.decode(tokenizer(text)["input_ids"])

inputs = processor(text=text, return_tensors="pt")

"""Tell the model to generate a spectrogram from the input text."""

spectrogram = model.generate_speech(inputs["input_ids"], speaker_embeddings)

plt.figure()
plt.imshow(spectrogram.T)
plt.show()

"""Finally, use the vocoder to turn the spectrogram into sound."""

with torch.no_grad():
    speech = vocoder(spectrogram)

from IPython.display import Audio
Audio(speech.numpy(), rate=16000)

import soundfile as sf
sf.write("output.wav", speech.numpy(), samplerate=16000)

